# ─────────────────────────────────────────────
# MASTER PROCESS CONTROL
# ─────────────────────────────────────────────
worker_processes 1;
# how many processes to spawn for nginx in the os

events {
  worker_connections 1024;
      # Max number of simultaneous connections per worker.
      # 1024 is good for light loads. You can raise this for high-concurrency.
      # For 4 cores and 1024 each, up to ~4000 concurrent connections.
  use epoll;
  # use epoll for linux
}

# ─────────────────────────────────────────────
# HTTP SERVER BLOCK
# ─────────────────────────────────────────────

http {
    include mime.types;

    default_type application/json;
    # If NGINX doesn’t recognize the file type, default to this.
    # Useful for proxying JSON APIs like we're doing in this example.

    sendfile on;
    # Enables zero-copy file transfer (sendfile syscall)
        # Highly efficient for serving static files
        # Not impactful in proxying use cases — but harmless and common
    keepalive_timeout 65;
        # How long to keep a connection alive (in seconds)
        # Longer = better reuse (good for clients using HTTP keep-alive)
        # Default is 75. 65 is common to avoid mismatches with upstream timeouts.

 # ─────────────────────────────────────────
    # LOGGING
    # ─────────────────────────────────────────

    access_log logs/access.log;
    # Log all HTTP requests with status, IP, duration, etc.
    # Use /dev/stdout instead if Dockerized.

    error_log logs/error.log;
    # Log startup errors, routing issues, syntax problems, etc.

    # ─────────────────────────────────────────
    # RATE LIMITING (PER-IP)
    # ─────────────────────────────────────────
 limit_req_zone $binary_remote_addr zone=per_ip_limit:10m rate=10r/s;
    # Define a "zone" in memory to track request rates per client IP.
    # $binary_remote_addr: more efficient binary form of client IP.
    # zone=per_ip_limit:10m → uses 10MB of memory to store IP state.
    # rate=10r/s → allows 10 requests per second per IP (with optional burst).

    upstream graph_gw_backend {
        server 127.0.0.1:8081;
    }
    upstream svc_next_backend {
        server 127.0.0.1:8082;
        # Defines a named backend cluster — here, it's just one service
        # This will be referred to in `proxy_pass`
        # Later, you can add load balancing here (e.g., multiple servers)
    }

#         # Determine if version=2 is set in query string
#         map $arg_version $route_to_v2 {
#             default       0;
#             2             1;
#         }

        # Set a variable to the chosen backend path
        map $arg_version $target_backend {
            default                http://graph_gw_backend;
            2                      http://svc_next_backend;
        }

        server {
            listen 8080;
            # Port NGINX listens on — this becomes your local API gateway port.
            # e.g., curl http://localhost:8080/graphql

            location /health {
                # Handle all requests to /health

                limit_req zone=per_ip_limit burst=5 nodelay;
                # Enforce the rate limiting:
                # burst=5 → allow short bursts above rate limit
                # nodelay → exceed burst = 429 immediately (no queuing)

                proxy_pass $target_backend$request_uri;

                #proxy_pass http://svc_next_backend/health;
                # Actual backend call — maps to http://127.0.0.1:8082/health
                # Can later be changed to: proxy_pass http://svc_next_backend$request_uri;

                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
                # Pass original client info to backend service (svc-next)
                # Required for logs, auth, rate-limiting based on real IP, etc.
            }
        }
    }
}